{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymising Research Data\n",
    "\n",
    "A practical Python implementation of Guidelines from the [UK Data Service](https://www.ukdataservice.ac.uk/manage-data/legal-ethical/anonymisation)\n",
    "\n",
    "## Pandas\n",
    "\n",
    "This notebook will also serve as an introduction to the Python Library [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html). **Pandas** is a library for handling tabular data. Its alot more robust for data that is not just numbers than **NumPy**. Its main data format is the **DataFrame**, which is like a 2D array, but with column names and indexes for the rows. \n",
    "\n",
    "Normally (in a ``real experiment``), we'd have the data saved in something like the **comma-separated variable (.csv)** format.\n",
    "\n",
    "We'll see how to do that, but before we'll see how I generated some fake data. \n",
    "\n",
    "If you are running this using Google Colab, you'll need to run the cell below to copy across some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY DO THIS IF YOU ARE USING GOOGLE COLAB (NOT LOCAL!)\n",
    "!git clone https://github.com/ual-cci/anonymising-data\n",
    "%cd anonymising-data\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library (pandas) and give it an alias (pd)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the faker library\n",
    "!pip3 install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating some fake data \n",
    "\n",
    "faker = Faker()\n",
    "data = []\n",
    "postcodes = pd.read_csv(\"postcodes.csv\").values.flatten()\n",
    "for _ in range(100):\n",
    "    profile = faker.simple_profile()\n",
    "    profile[\"address\"] = np.random.choice(postcodes)\n",
    "    profile[\"job\"] = faker.job()\n",
    "    profile[\"phone\"] = faker.phone_number()\n",
    "    profile[\"userid\"] = faker.uuid4()\n",
    "    profile[\"survey_answers\"] = np.random.randint(1,6,20)\n",
    "    profile[\"salary\"] = np.random.normal(22000,2000)\n",
    "    data.append(profile)\n",
    "\n",
    "#High earning outliers\n",
    "data[0][\"salary\"] = 120000\n",
    "data[1][\"salary\"] = 140000\n",
    "my_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.to_csv(\"my_fake_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiative Data\n",
    "\n",
    "### Remove direct identifiers from a dataset\n",
    "\n",
    "Such identifiers are often not necessary for secondary research.\n",
    "\n",
    "Lets first look at what information is in here that is **personally identifiable** but **not relevant** to research \n",
    "\n",
    "The ``.head()`` function lets us eye-ball the first few entries in the dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in .csv file, tell Pandas which column is contains dates\n",
    "data = pd.read_csv(\"my_fake_data.csv\", parse_dates = [\"birthdate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns we need to remove completely and just delete, some columns we need to remove but keep a reference to. \n",
    "\n",
    "We can use the Pandas function ``.drop()`` to completely lose some columns entirely. This is for things that are identifiable and not useful for sharing with other researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get rid off username, phone, userid\n",
    "data = data.drop([\"username\", \"phone\", \"userid\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove the name and email as they identifiable, **but** we want to keep a reference to them so we can get back and identify the participants if we need to later. \n",
    "\n",
    "We can save this reference separately in a **very secure and restricted place**. There is no reason for anyone but the lead researchers to ever have access to this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique() names \n",
    "names = data[\"name\"].unique()\n",
    "\n",
    "#New dictionaries to keep mappings\n",
    "id_to_name = {}\n",
    "name_to_id = {}\n",
    "\n",
    "#Go through each name\n",
    "for i, name in enumerate(names):\n",
    "    \n",
    "    #Make a new identifier\n",
    "    identifier = \"P\"+str(i)\n",
    "    \n",
    "    #Get the email\n",
    "    email = data[data[\"name\"] == name][\"mail\"].item()\n",
    "    \n",
    "    #Save the name and email against the identifier\n",
    "    id_to_name[identifier] = [name, email]\n",
    "    \n",
    "    #And reverse \n",
    "    name_to_id[name] = identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new column with the participant id\n",
    "data[\"participant_id\"] = [name_to_id[name] for name in data[\"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the reference file (keep this secure!)\n",
    "pd.DataFrame(id_to_name).T.to_csv(\"participant_lookup.csv\", header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the name and email columns \n",
    "data = data.drop([\"name\", \"mail\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate or reduce the precision \n",
    "\n",
    "You can do this for \n",
    "\n",
    "\n",
    "* Age \n",
    "\n",
    "    * Record birth year (not month, day)\n",
    "   \n",
    "\n",
    "* Place of Residence \n",
    "\n",
    "    * Record Postcode sectors (first 3 - 4 digits) \n",
    "\n",
    "\n",
    "### Dates\n",
    "\n",
    "In Pandas, the date is specified in its own data type so its quite easy to manipulate to, for example, remove precision information.\n",
    "\n",
    "We can reformat the column by giving it a new `format string`. Here, we give it ``'%Y'`` to tell it to just **keep the year only**\n",
    "\n",
    "You can find the documentation for formatting date strings in Python [here](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reformatting dates \n",
    "data[\"birthdate\"] = data[\"birthdate\"].dt.strftime('%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post codes\n",
    "\n",
    "For the post code we can remove the end of the postcode to reduce precision \n",
    "\n",
    "For UK Post codes, the first area location can be 2-4 characters. But, consistently we can achieve the anonymisation by removing the **last 3 characters**. \n",
    "\n",
    "Here we use the ``str.slice()`` function and use the **negative index** to say ``slice 3 from the end``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reformatting strings (delete last 3 characters)\n",
    "data[\"address\"] = data[\"address\"].str.slice(stop = -3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict upper or lower ranges \n",
    "\n",
    "We can hide outliers as these may be used to identify people known to to have atypical values \n",
    "\n",
    "For example, you can **top-code** or **bottom-code** high or low values respectively. This means grouping everyone above or below a given threshold into one category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold and replace \n",
    "top_limit = 70000\n",
    "column = \"salary\"\n",
    "row_indexer = data[column] > top_limit\n",
    "\n",
    "#Using .loc and a mask as a [row_indexer, col_indexer]\n",
    "data.loc[row_indexer, column] = top_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media Data \n",
    "\n",
    "### Audio (Voices)\n",
    "\n",
    "Sometimes its necessary to disguise voices in audio recordings. We can use the `librosa` package to re-pitch a voice but keep the time information the same \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install audio packages\n",
    "!pip3 install librosa\n",
    "!pip3 install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Using your own files - Colab Users\n",
    "\n",
    "You can run these examples using the test files we have given you by just leaving the paths as they are. \n",
    "\n",
    "You can use **your own files** by mounting your Google Drive by running the cell below. You'll then be able to access files from your Google Drive at the root path ``/content/drive``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY DO THIS IF YOU ARE USING GOOGLE COLAB\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick paths\n",
    "file_path = 'voice.wav'\n",
    "output_file = 'repitched_voice.wav'\n",
    "\n",
    "#Load in file\n",
    "y, sr = librosa.load(file_path)\n",
    "repitched_audio = librosa.effects.pitch_shift(y, sr, n_steps = -4)\n",
    "\n",
    "#Save altered file\n",
    "sf.write(output_file, repitched_audio, sr, subtype='PCM_24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faces - Images\n",
    "\n",
    "We can also use the ``OpenCV`` package to find faces and apply a ``Gaussian blur``\n",
    "\n",
    "First we see how this can be applied to images one at a time, and to all images in a given folder using Pythons ``os.walk`` function. This does a recursive walk through all the folders from a given top directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install computer vision packages\n",
    "!pip3 install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://www.geeksforgeeks.org/how-to-blur-faces-in-images-using-opencv-in-python/\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_folder = \"images/\"\n",
    "for root, dirs, files in os.walk(top_folder, topdown=False):\n",
    "    for name in files:\n",
    "        if not name == \".DS_Store\":\n",
    "            image_path = os.path.join(root, name)\n",
    "            print(image_path)\n",
    "            # Reading an image using OpenCV\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            face_detect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "            face_data = face_detect.detectMultiScale(image, 1.3, 2)\n",
    "\n",
    "            # Draw rectangle around the faces which is our region of interest (ROI)\n",
    "            for (x, y, w, h) in face_data:\n",
    "                roi = image[y:y+h, x:x+w]\n",
    "                # applying a gaussian blur over this new rectangle area\n",
    "                roi = cv2.GaussianBlur(roi, (23, 23), 30)\n",
    "                # impose this blurred image on original image to get final image\n",
    "                image[y:y+roi.shape[0], x:x+roi.shape[1]] = roi\n",
    "\n",
    "            #Convert back to RGB    \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(image_path[:-4]+\"_blurred.jpg\", image)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faces - Video\n",
    "\n",
    "Now we see how we can go through each frame one by one and blur the faces\n",
    "\n",
    "We also put the audio back at the bottom using the ``ffmpeg`` library, and optionally repitch \n",
    "\n",
    "To install ffmpeg, the easiest way is through [HomeBrew](https://brew.sh/) (which you might have to install as well!) \n",
    "\n",
    "Use ``brew install ffmpeg``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = \"louis.mp4\"\n",
    "output_video_path = \"anon.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "#Get input video meta data\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\"blurred_temp.mp4\", fourcc, fps, (width,  height))\n",
    "face_detect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "#For every video frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of stream\")\n",
    "        break\n",
    "    \n",
    "    #Blur face\n",
    "    face_data = face_detect.detectMultiScale(frame, 1.2, 1)\n",
    "    for (x, y, w, h) in face_data:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        roi = cv2.GaussianBlur(roi, (23, 23), 30)\n",
    "        frame[y:y+roi.shape[0], x:x+roi.shape[1]] = roi\n",
    "        \n",
    "    #Write new frame\n",
    "    out.write(frame)\n",
    "    \n",
    "out.release() \n",
    "\n",
    "#Extract original audio\n",
    "os.system(\"ffmpeg -i \" + input_video_path + \" -q:a 0 -map a audio_temp.wav\")\n",
    "\n",
    "#REPITCH AUDIO (comment out if not wanted!)\n",
    "y, sr = librosa.load(\"audio_temp.wav\")\n",
    "repitched_audio = librosa.effects.pitch_shift(y, sr, n_steps=-4)\n",
    "sf.write(\"audio_temp.wav\", repitched_audio, sr, subtype='PCM_24')\n",
    "\n",
    "#Put audio back on output video\n",
    "os.system(\"ffmpeg -i  blurred_temp.mp4 -i audio_temp.wav -map 0:v:0 -map 1:a:0 \" + output_video_path)\n",
    "\n",
    "#Delete temp files\n",
    "os.system(\"rm audio_temp.wav blurred_temp.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
